{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfd\n",
      "{0: 'w0', 1: 'w1'}\n",
      "dict_keys(['2', '1']) \n",
      " ----\n",
      " 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x12b6350b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 522, in __del__\n",
      "    self.close()\n",
      "  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\", line 1262, in close\n",
      "    self._default_session.__exit__(None, None, None)\n",
      "  File \"/Applications/anaconda/lib/python3.5/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/Applications/anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3536, in get_controller\n",
      "    % type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'weakref'> objects\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-561fe3f0e301>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mt1\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-d5f4726c71df>\u001b[0m in \u001b[0;36merror\u001b[0;34m(X, Y)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print('dfd')\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "a = np.array([[1.,2.],[3,4]])\n",
    "#print(tf.l2_normalize())\n",
    "\n",
    "\n",
    "dict_test={}\n",
    "for i in range(2):\n",
    "    dict_test[i] = 'w'+str(i)\n",
    "print(dict_test)\n",
    "\n",
    "\n",
    "a = {'1':'11','2':'22'}\n",
    "def Dic(**c):\n",
    "    print(c.keys(),'\\n ----\\n',c['1'])\n",
    "Dic(**a)\n",
    "w = np.array([[2.,1.],[3.,4.]])\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "'''\n",
    "def error(X,Y):\n",
    "    result = np.zeros_like(X)\n",
    "    n= X.shape[1]\n",
    "    for i in range(n):\n",
    "        result[i] = X[i]*X[i]-2*X[i]*Y[i]+Y[i]*Y[i]\n",
    "'''\n",
    "        \n",
    "t1  = tf.constant([[2.2,3.0],[2.,1.1]])\n",
    "t2 = tf.constant([[2.2,2.],[1.,1.]])\n",
    "\n",
    "print(t.eval())\n",
    "print(c.eval())\n",
    "\n",
    "x = [[1.,2.,1.],[1.,1.,1.]]    # a 2D matrix as input to softmax\n",
    "y = tf.nn.softmax(x)           # this is the softmax function\n",
    "                               # you can have anything you like here\n",
    "u = y.eval()\n",
    "print(u)\n",
    "sess.close()\n",
    "\n",
    "#print(tf.reduce_mean(w))\n",
    "#print(tf.reduce_mean(t),keep_dims = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "iteration 0\t accuracy: 0.407\n",
      "iteration 2000\t accuracy: 0.918\n",
      "iteration 4000\t accuracy: 0.919\n",
      "iteration 6000\t accuracy: 0.922\n",
      "iteration 8000\t accuracy: 0.923\n",
      "iteration 10000\t accuracy: 0.922\n",
      "iteration 12000\t accuracy: 0.924\n",
      "iteration 14000\t accuracy: 0.924\n",
      "iteration 16000\t accuracy: 0.926\n",
      "iteration 18000\t accuracy: 0.924\n",
      "iteration 20000\t accuracy: 0.925\n"
     ]
    }
   ],
   "source": [
    "def logistic_regression(x_):\n",
    "    # create the actual model\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=1e-4)}\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        W = tf.get_variable('W', shape=[784, 10])\n",
    "        b = tf.get_variable('b', shape=[10])\n",
    "        y_logits = tf.matmul(x_, W) + b\n",
    "    return y_logits\n",
    "\n",
    "def test_classification(model_function, learning_rate=0.1):\n",
    "    # import data\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "\n",
    "    with tf.Graph().as_default() as g:\n",
    "        # where are you going to allocate memory and perform computations\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            \n",
    "            # define model \"input placeholders\", i.e. variables that are\n",
    "            # going to be substituted with input data on train/test time\n",
    "            x_ = tf.placeholder(tf.float32, [None, 784])\n",
    "            y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "            y_logits = model_function(x_)\n",
    "            \n",
    "            # naive implementation of loss:\n",
    "            # > losses = y_ * tf.log(tf.nn.softmax(y_logits))\n",
    "            # > tf.reduce_mean(-tf.reduce_sum(losses, 1))\n",
    "            # can be numerically unstable.\n",
    "            #\n",
    "            # so here we use tf.nn.softmax_cross_entropy_with_logits on the raw\n",
    "            # outputs of 'y', and then average across the batch.\n",
    "            \n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_logits)\n",
    "            cross_entropy_loss = tf.reduce_mean(losses)\n",
    "            train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy_loss)\n",
    "            \n",
    "            y_pred = tf.argmax(tf.nn.softmax(y_logits), dimension=1)\n",
    "            correct_prediction = tf.equal(y_pred, tf.argmax(y_, 1))\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    with g.as_default(), tf.Session() as sess:\n",
    "        # that is how we \"execute\" statements \n",
    "        # (return None, e.g. init() or train_op())\n",
    "        # or compute parts of graph defined above (loss, output, etc.)\n",
    "        # given certain input (x_, y_)\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # train\n",
    "        for iter_i in range(20001):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "            sess.run(train_step, feed_dict={x_: batch_xs, y_: batch_ys})\n",
    "            \n",
    "            # test trained model\n",
    "            if iter_i % 2000 == 0:\n",
    "                tf_feed_dict = {x_: mnist.test.images, y_: mnist.test.labels}\n",
    "                acc_value = sess.run(accuracy, feed_dict=tf_feed_dict)\n",
    "                print('iteration %d\\t accuracy: %.3f'%(iter_i, acc_value))\n",
    "                \n",
    "test_classification(logistic_regression, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mlp2(x, hidden_sizes, activation_fn=tf.nn.relu):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=1e-4)}\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        W1 = tf.get_variable('W1', shape=[784, hidden_sizes[0]])\n",
    "        b1 = tf.get_variable('b1', shape=[hidden_sizes[0]])\n",
    "        layer1 = tf.matmul(x, W1) + b1\n",
    "        layer1 = activation_fn(layer1)\n",
    "        W2 = tf.get_variable('W2', shape=[hidden_sizes[0], hidden_sizes[1]])\n",
    "        b2 = tf.get_variable('b2', shape=[hidden_sizes[1]])\n",
    "        y_logits = tf.matmul(layer1,W2)+b2\n",
    "    return y_logits   \n",
    "\n",
    "def mlp(x, hidden_sizes, activation_fn=tf.nn.relu):\n",
    "    if not isinstance(hidden_sizes, (list, tuple)):\n",
    "        raise ValueError(\"hidden_sizes must be a list or a tuple\")\n",
    "    scope_args = {'initializer': tf.random_normal_initializer(stddev=1e-4)}\n",
    "    n = len(hidden_sizes)\n",
    "    W_hidden = []\n",
    "    b_hidden = []\n",
    "    with tf.variable_scope(\"weights\", **scope_args):\n",
    "        W1 = tf.get_variable('W1', shape=[784, hidden_sizes[0]])\n",
    "        b1 = tf.get_variable('b1', shape=[hidden_sizes[0]])\n",
    "        layer1 = tf.matmul(x, W1) + b1\n",
    "        layer1 = activation_fn(layer1)\n",
    "        \n",
    "        for i in range(1,n):\n",
    "            \n",
    "            W_hidden.append(tf.get_variable('W'+str(i+1), shape=[hidden_sizes[i], hidden_sizes[i+1]]))\n",
    "            b_hidden.append(tf.get_variable('b'+str(i+1), shape=[hidden_sizes[i+1]]))\n",
    "            layer2 = tf.matmul(layer1, W_hidden[i]) + b_hidden[i]\n",
    "            layer = activation_fn(layer2)\n",
    "            layer1 = layer\n",
    "  \n",
    "        y_logits = layer1\n",
    "    return y_logits   \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./datasets/mnist/t10k-labels-idx1-ubyte.gz\n",
      "iteration 0\t accuracy: 0.096\n",
      "iteration 2000\t accuracy: 0.930\n",
      "iteration 4000\t accuracy: 0.953\n",
      "iteration 6000\t accuracy: 0.963\n",
      "iteration 8000\t accuracy: 0.966\n",
      "iteration 10000\t accuracy: 0.969\n",
      "iteration 12000\t accuracy: 0.971\n",
      "iteration 14000\t accuracy: 0.972\n",
      "iteration 16000\t accuracy: 0.973\n",
      "iteration 18000\t accuracy: 0.975\n",
      "iteration 20000\t accuracy: 0.973\n"
     ]
    }
   ],
   "source": [
    "test_classification(lambda x: mlp2(x, [64, 10], activation_fn=tf.nn.relu), learning_rate=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#################################################################\n",
    "# Inputs: arr1 and arr2 have shape [batch_size, hidden_sizes[-1]]\n",
    "# Output: return tensor of shape [batch_size, ], the cosine \n",
    "#         similarity betwwen arr1 and arr2\n",
    "# \n",
    "# Hint: use tf.l2_normalize, tf.mul, tf.reduce_sum\n",
    "#################################################################\n",
    "\n",
    "\n",
    "\n",
    "def cosine_similarity(arr1, arr2):\n",
    "    ###################################\n",
    "    ####     PUT YOUR CODE HERE    ####\n",
    "    ###################################\n",
    "    a = tf.mul(arr1,arr2)\n",
    "    return a/(tf.l2_normalize(arr1)*tf.l2_normalize(arr2))\n",
    "    \n",
    "    \n",
    "#################################################################\n",
    "# Inputs: mlp_args is a dictionary of arguments to the mlp() \n",
    "#         function. \n",
    "#         Example: mlp_args = {'hidden_sizes':[64, 64, 32]}\n",
    "#################################################################\n",
    "def build_model(mlp_args):\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            x1 = tf.placeholder(tf.float32, [None, 784])\n",
    "            x2 = tf.placeholder(tf.float32, [None, 784])\n",
    "            y = tf.placeholder(tf.float32, [None])\n",
    "\n",
    "            with tf.variable_scope(\"siamese\") as var_scope:\n",
    "                x_repr1 = mlp(x1, **mlp_args)  # hidden representation of x1\n",
    "                var_scope.reuse_variables()    # weight sharing! \n",
    "                x_repr2 = mlp(x2, **mlp_args)  # hidden representation of x2\n",
    "                logits = cosine_similarity(x_repr1, x_repr2)  # similarity\n",
    "                \n",
    "            ###################################\n",
    "            ####     PUT YOUR CODE HERE    ####\n",
    "            ###################################\n",
    "            \n",
    "            # define scalar: loss \n",
    "            loss = tf.pow(tf.sub(x_repr1, x_repr2), 2)\n",
    "            loss = tf.reduce_sum(loss, 1)\n",
    "            # define vector: y_prob as sigmoid(cosine_similarity)\n",
    "            \n",
    "            y_prob = tf.sigmoid(cosin_similarity)\n",
    "            # define vector: y_pred as sign(cosine_similarity)\n",
    "            y_pred = tf.sign(consine_similarity)\n",
    "            # define scalar: accuracy as the fraction of correct predictions\n",
    "            accuracy = \n",
    "            \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data preparation\n",
    "def mnist_siamese_dataset_iterator(batch_size, dataset_name):\n",
    "    assert dataset_name in ['train', 'test']\n",
    "    assert batch_size > 0 or batch_size == -1 # -1 for entire dataset\n",
    "    mnist = input_data.read_data_sets('./datasets/mnist/', one_hot=True)\n",
    "    dataset = getattr(mnist, dataset_name)\n",
    "    \n",
    "    while True:\n",
    "        if batch_size > 0:\n",
    "            X1, y1 = dataset.next_batch(batch_size)\n",
    "            X2, y2 = dataset.next_batch(batch_size)\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y\n",
    "        else:\n",
    "            X1 = dataset.images\n",
    "            idx = np.arange(len(X1))\n",
    "            np.random.shuffle(idx)\n",
    "            X2 = X1[idx]\n",
    "            y1 = dataset.labels\n",
    "            y2 = y1[idx]\n",
    "            y = np.argmax(y1, axis=1) == np.argmax(y2, axis=1)\n",
    "            yield X1, X2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError:\n",
    "    print('This is Python 3')\n",
    "\n",
    "def run_training(model_dict, train_data_iterator, test_full_iter, \n",
    "                 train_full_iter, n_iter=1000, print_every=100):\n",
    "    with model_dict['graph'].as_default():\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.1)\n",
    "        train_op = optimizer.minimize(model_dict['loss'])\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for iter_i, data_batch in zip(range(n_iter), train_data_iterator):\n",
    "                batch_feed_dict = dict(zip(model_dict['inputs'], data_batch))\n",
    "                sess.run(train_op, feed_dict=batch_feed_dict)\n",
    "                if iter_i % print_every == 0:\n",
    "                    print_zip_iter = zip([test_full_iter, train_full_iter], ['test', 'train'])\n",
    "                    for data_iterator, data_name in print_zip_iter:\n",
    "                        test_batch = next(data_iterator)\n",
    "                        batch_feed_dict = dict(zip(model_dict['inputs'], test_batch))\n",
    "                        to_compute = [model_dict['accuracy'], model_dict['loss']]\n",
    "                        acc_value, loss_val = sess.run(to_compute, batch_feed_dict)\n",
    "                        fmt = (iter_i, acc_value, loss_val)\n",
    "                        print(data_name, 'iteration %d\\t accuracy: %.3f, loss: %.3f'%fmt)\n",
    "\n",
    "train_data_iterator = mnist_siamese_dataset_iterator(100, 'train')\n",
    "test_full_iter = mnist_siamese_dataset_iterator(-1, 'test')\n",
    "train_full_iter = mnist_siamese_dataset_iterator(-1, 'train')\n",
    "\n",
    "mlp_args = {'hidden_sizes':[64, 64, 32]}\n",
    "model = build_model(mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_build_model(mlp_args):\n",
    "    \n",
    "    ###################################\n",
    "    ####     PUT YOUR CODE HERE    ####\n",
    "    ###################################\n",
    "    \n",
    "    return {'graph': g, 'inputs': [x1, x2, y], 'pred': y_pred, 'logits': logits,\n",
    "            'prob': y_prob, 'loss': loss, 'accuracy': accuracy}\n",
    "\n",
    "my_mlp_args = {}  # you can define your own arguments\n",
    "model = my_build_model(my_mlp_args)\n",
    "run_training(model, train_data_iterator, test_full_iter, train_full_iter)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
